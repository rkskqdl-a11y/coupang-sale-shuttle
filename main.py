import os, hmac, hashlib, time, requests, json, random, re
from datetime import datetime
from urllib.parse import urlencode

# [1. ì„¤ì • ì •ë³´]
ACCESS_KEY = os.environ.get('COUPANG_ACCESS_KEY', '').strip()
SECRET_KEY = os.environ.get('COUPANG_SECRET_KEY', '').strip()
GEMINI_KEY = os.environ.get('GEMINI_API_KEY', '').strip()
SITE_URL = "https://rkskqdl-a11y.github.io/coupang-sale-shuttle"

def generate_ai_content(product_name):
    """ğŸ’ ì œë¯¸ë‚˜ì´ AIë¡œ 1,000ì ì´ìƒì˜ ê³ í’ˆì§ˆ ì¹¼ëŸ¼ ìƒì„±"""
    if not GEMINI_KEY: return "ë¶„ì„ ë°ì´í„° ì¤€ë¹„ ì¤‘"
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_KEY}"
    prompt = f"ìƒí’ˆ '{product_name}'ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ë¶„ì„ ì¹¼ëŸ¼ì„ 1,000ì ì´ìƒ ì¥ë¬¸ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. <h3> íƒœê·¸ë¥¼ í™œìš©í•´ ì„¹ì…˜ì„ ë‚˜ëˆ„ê³  HTMLë§Œ ì‚¬ìš©í•´. 'í•´ìš”ì²´'ë¡œ ì‘ì„±í•˜ê³  'í• ì¸' ì–¸ê¸‰ì€ ê¸ˆì§€."
    try:
        response = requests.post(url, json={"contents": [{"parts": [{"text": prompt}]}]}, timeout=60)
        return response.json()['candidates'][0]['content']['parts'][0]['text'].replace("\n", "<br>")
    except:
        return f"<h3>ğŸ” ì œí’ˆ ìƒì„¸ ë¶„ì„</h3>{product_name}ì€ ì‹¤ìš©ì„±ê³¼ ì™„ì„±ë„ê°€ ë§¤ìš° ë›°ì–´ë‚œ ì œí’ˆì…ë‹ˆë‹¤."

def get_authorization_header(method, path, query_string):
    """ğŸ’ ì‚¬ìš©ìë‹˜ì´ ì„±ê³µí–ˆë˜ ì¸ì¦ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤."""
    datetime_gmt = time.strftime('%y%m%dT%H%M%SZ', time.gmtime())
    message = datetime_gmt + method + path + query_string
    signature = hmac.new(bytes(SECRET_KEY, 'utf-8'), msg=bytes(message, 'utf-8'), digestmod=hashlib.sha256).hexdigest()
    return f"CEA algorithm=HmacSHA256, access-key={ACCESS_KEY}, signed-date={datetime_gmt}, signature={signature}"

def fetch_data(keyword, page):
    """ğŸ’ íŠ¹ì • í˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        DOMAIN = "https://api-gateway.coupang.com"
        path = "/v2/providers/affiliate_open_api/apis/openapi/v1/products/search"
        params = {"keyword": keyword, "limit": 20, "page": page}
        query_string = urlencode(params)
        url = f"{DOMAIN}{path}?{query_string}"
        headers = {"Authorization": get_authorization_header("GET", path, query_string), "Content-Type": "application/json"}
        response = requests.get(url, headers=headers, timeout=15)
        return response.json().get('data', {}).get('productData', [])
    except: return []

def main():
    os.makedirs("posts", exist_ok=True)
    
    # ğŸ’ ê²€ìƒ‰ì˜ ì¤‘ì‹¬ì´ ë  í•µì‹¬ í‚¤ì›Œë“œ (í•˜ë‚˜ì”© ìˆœì°¨ì ìœ¼ë¡œ ì •ë³µí•©ë‹ˆë‹¤)
    target_keyword = "ê°€ìŠµê¸°" # ì‚¬ìš©ìë‹˜ì´ ë§ì”€í•˜ì‹  í‚¤ì›Œë“œ ì˜ˆì‹œ
    
    existing_ids = {f.split('_')[-1].replace('.html', '') for f in os.listdir("posts") if '_' in f}
    success_count, max_target = 0, 10
    
    print(f"ğŸ•µï¸ í˜„ì¬ {len(existing_ids)}ê°œ ë…¸ì¶œ ì¤‘. '{target_keyword}' ì¹´í…Œê³ ë¦¬ ìˆœì°¨ ìˆ˜ìƒ‰ ì‹œì‘!")

    # ğŸ’ 1í˜ì´ì§€ë¶€í„° 50í˜ì´ì§€ê¹Œì§€ ìˆœì„œëŒ€ë¡œ íƒìƒ‰
    for page in range(1, 51):
        if success_count >= max_target: break
        
        print(f"ğŸ” [í˜ì´ì§€ {page}] ë¶„ì„ ì¤‘...")
        products = fetch_data(target_keyword, page)
        
        if not products:
            print(f"   âš ï¸ {page}í˜ì´ì§€ì— ë” ì´ìƒ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # í•´ë‹¹ í˜ì´ì§€ì˜ ëª¨ë“  ìƒí’ˆì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        new_items_on_page = [item for item in products if str(item['productId']) not in existing_ids]
        
        if not new_items_on_page:
            print(f"   â© {page}í˜ì´ì§€ëŠ” ì´ë¯¸ ëª¨ë‘ í¬ìŠ¤íŒ…ëœ ìƒí’ˆì…ë‹ˆë‹¤. ë‹¤ìŒ í˜ì´ì§€ë¡œ!")
            continue

        # ìƒˆ ìƒí’ˆì´ ìˆë‹¤ë©´ ë°œí–‰ ì‹œì‘
        for item in new_items_on_page:
            p_id = str(item['productId'])
            p_name = item['productName']
            
            print(f"   âœ¨ ë°œê²¬! [{success_count+1}/10] {p_name[:20]}... ë°œí–‰ ì¤‘")
            ai_content = generate_ai_content(p_name)
            img, price = item['productImage'].split('?')[0], format(item['productPrice'], ',')
            
            filename = f"posts/{datetime.now().strftime('%Y%m%d')}_{p_id}.html"
            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"<!DOCTYPE html><html lang='ko'><head><meta charset='UTF-8'><meta name='viewport' content='width=device-width, initial-scale=1.0'><title>{p_name} ë¦¬ë·°</title><style>body{{font-family:sans-serif; background:#f8f9fa; padding:20px; color:#333; line-height:2.2;}} .card{{max-width:750px; margin:auto; background:white; padding:50px; border-radius:30px; box-shadow:0 20px 50px rgba(0,0,0,0.05);}} h3{{color:#e44d26; margin-top:40px; border-left:6px solid #e44d26; padding-left:20px;}} img{{width:100%; border-radius:20px; margin:30px 0;}} .price-box{{text-align:center; background:#fff5f2; padding:30px; border-radius:20px; margin:40px 0;}} .p-val{{font-size:2.5rem; color:#e44d26; font-weight:bold;}} .buy-btn{{display:block; background:#e44d26; color:white; text-align:center; padding:25px; text-decoration:none; border-radius:60px; font-weight:bold; font-size:1.3rem;}}</style></head><body><div class='card'><h2>{p_name}</h2><img src='{img}'><div class='content'>{ai_content}</div><div class='price-box'><div class='p-val'>{price}ì›</div></div><a href='{item['productUrl']}' class='buy-btn'>ğŸ›ï¸ ìƒì„¸ ì •ë³´ í™•ì¸í•˜ê¸°</a></div></body></html>")
            
            existing_ids.add(p_id)
            success_count += 1
            time.sleep(30) # API ì•ˆì •ì„±
            if success_count >= max_target: break

    # ğŸ’ [ì‚¬ì´íŠ¸ë§µ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í•´ê²°] êµ¬ê¸€ í‘œì¤€ ê·œê²© ì‚½ì…
    files = sorted([f for f in os.listdir("posts") if f.endswith(".html")], reverse=True)
    now_iso = datetime.now().strftime("%Y-%m-%d")
    
    sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n'
    sitemap += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
    sitemap += f'  <url><loc>{SITE_URL}/</loc><lastmod>{now_iso}</lastmod><priority>1.0</priority></url>\n'
    for f in files:
        sitemap += f'  <url><loc>{SITE_URL}/posts/{f}</loc><lastmod>{now_iso}</lastmod></url>\n'
    sitemap += '</urlset>'
    
    with open("sitemap.xml", "w", encoding="utf-8") as f: f.write(sitemap.strip())
    with open("robots.txt", "w", encoding="utf-8") as f: f.write(f"User-agent: *\nAllow: /\nSitemap: {SITE_URL}/sitemap.xml\n")
    
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(f"<!DOCTYPE html><html lang='ko'><head><meta charset='UTF-8'><title>ì¿ íŒ¡ ì „ìˆ˜ ì¡°ì‚¬ ë§¤ê±°ì§„</title><style>body{{font-family:sans-serif; background:#f0f2f5; padding:20px;}} .grid{{display:grid; grid-template-columns:repeat(auto-fill, minmax(320px, 1fr)); gap:30px;}} .card{{background:white; padding:30px; border-radius:25px; text-decoration:none; color:#333; box-shadow:0 10px 20px rgba(0,0,0,0.05); height:160px; display:flex; flex-direction:column; justify-content:space-between;}} .title{{font-weight:bold; overflow:hidden; text-overflow:ellipsis; display:-webkit-box; -webkit-line-clamp:3; -webkit-box-orient:vertical; font-size:0.95rem;}}</style></head><body><h1 style='text-align:center;'>ğŸš€ ì‹¤ì‹œê°„ ì¿ íŒ¡ ì „ ìƒí’ˆ ë…¸ì¶œ</h1><div class='grid'>")
        for file in files[:150]:
            try:
                with open(f"posts/{file}", 'r', encoding='utf-8') as fr:
                    title = re.search(r'<title>(.*?)</title>', fr.read()).group(1).replace(" ë¦¬ë·°", "")
                f.write(f"<a class='card' href='posts/{file}'><div class='title'>{title}</div><div style='color:#e44d26; font-weight:bold;'>ì¹¼ëŸ¼ ì½ê¸° ></div></a>")
            except: continue
        f.write("</div></body></html>")
    
    print(f"ğŸ ì‘ì—… ì¢…ë£Œ! ì´ {len(files)}ê°œ ë…¸ì¶œ ì¤‘.")

if __name__ == "__main__":
    main()
